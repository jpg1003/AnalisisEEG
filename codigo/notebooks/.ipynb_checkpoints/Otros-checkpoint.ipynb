{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362412a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\") #Evitar escribir warinings con ensembles\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_experimento_4():\n",
    "    # Divido los datos en data y target\n",
    "    y = experimento_4_nor['Key'].values\n",
    "    X = experimento_4_nor.drop(columns=['Key']).values\n",
    "\n",
    "    return {'data': X, 'target': y}\n",
    "\n",
    "# Defino los scorers\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "bal_acc_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Defino los conjuntos de datos\n",
    "datasets = [load_experimento_4()]\n",
    "nombres = [\"exp_4\"]\n",
    "tablas = [\"Accuracy\", \"Balanced Accuracy\"]\n",
    "scoring = [acc_scorer, bal_acc_scorer]\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "lista = [] \n",
    "\n",
    "import time\n",
    "inicio = time.time()\n",
    "\n",
    "for tabla, score in zip(tablas, scoring): # Por cada tabla y su correcpondiete score\n",
    "    \n",
    "    dataset_names = []  # Lista para almacenar los nombres de los datasets\n",
    "    knn_means = []      # Lista para almacenar las medias de KNeighborsClassifier\n",
    "    tree_means = []     # Lista para almacenar las medias de DecisionTreeClassifier\n",
    "    rf_means = []       # Lista para almacenar las medias de Random Forest\n",
    "    \n",
    "    for dataset, nombre in zip(datasets, nombres): # Por cada dataset y su correcpondiete nombre\n",
    "      \n",
    "        # Calculo las  metricas       \n",
    "        knn = cross_val_score(KNeighborsClassifier(), X, y, cv=CV, scoring=score)      \n",
    "        tree = cross_val_score(DecisionTreeClassifier(), X, y, cv=CV, scoring=score)  \n",
    "        rf = cross_val_score(RandomForestClassifier(), X, y, cv=CV, scoring=score)\n",
    "\n",
    "        # Calculo la media de las metricas anteriores\n",
    "        knn_mean = knn.mean()\n",
    "        tree_mean = tree.mean()\n",
    "        rf_mean = rf.mean()\n",
    "\n",
    "        # Agrego las medias de los resultados a cada lista\n",
    "        dataset_names.append(nombre)\n",
    "        knn_means.append(knn_mean)\n",
    "        tree_means.append(tree_mean)\n",
    "        rf_means.append(rf_mean)\n",
    "\n",
    "    # Creo dataframes a partir de las listas con las medias de los resultados\n",
    "    resultados = pd.DataFrame({\n",
    "    'Data': dataset_names,\n",
    "    'KNN': knn_means,\n",
    "    'TREE': tree_means,\n",
    "    'RF': rf_means,\n",
    "    })\n",
    "\n",
    "    # Agrego el dataframe creado con los resultados a la lista\n",
    "    lista.append(resultados)\n",
    "  \n",
    "# Saco por pantalla las tablas de cada metrica\n",
    "fin = time.time()\n",
    "print(fin-inicio)\n",
    "for index, (dataframe, tabla) in enumerate(zip(lista, tablas)):\n",
    "    print(tabla)                              \n",
    "    display(dataframe)\n",
    "    print()\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f69f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@martacasdelg/c%C3%B3mo-identificar-y-tratar-outliers-con-python-bf7dd530fc3\n",
    "\n",
    "# Calculate z-score for each data point and compute its absolute value\n",
    "z_scores = zscore(experimento_4_filtrado['Delta'])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "\n",
    "# Select the outliers using a threshold of 3\n",
    "outliers = experimento_4_filtrado[abs_z_scores > 3]\n",
    "#print(f'Number of outliers: {len(outliers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3742ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### umbral Zcore https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule\n",
    "\n",
    "### Con 3   99.7%\n",
    "### Con 3.5 99.95%\n",
    "### Con 4   99.993%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_outliners = []\n",
    "\n",
    "# Iterar sobre los dataframes y aplicar el proceso a cada uno de ellos\n",
    "for exp in exp_filtrados:\n",
    "    \n",
    "    # Calcular el z-score para cada valor en el DataFrame\n",
    "    z_scores = stats.zscore(exp)\n",
    "\n",
    "    # Definir un umbral para los z-scores que considerarás como outliers\n",
    "    umbral_zscore = 3\n",
    "\n",
    "    # Detectar outliers basados en el z-score (valores cuyo valor absoluto de z-score es mayor que el umbral)\n",
    "    outliers = (abs(z_scores) > umbral_zscore)\n",
    "\n",
    "    # Contar el número de outliers en cada columna\n",
    "    num_outliers_por_columna = outliers.sum(axis=0)\n",
    "\n",
    "    # Obtener las filas y sus índices que son outliers en cada columna\n",
    "    outliers_por_columna = {}\n",
    "    for column in exp.columns:\n",
    "        outliers_index = outliers[outliers[column]].index\n",
    "        outliers_por_columna[column] = outliers_index\n",
    "\n",
    "    # Imprimir el número de outliers por cada columna\n",
    "    print(\"Número de outliers por columna:\")\n",
    "    print(num_outliers_por_columna)\n",
    "\n",
    "    # Imprimir las filas con sus índices que son outliers en cada columna\n",
    "    print(\"\\nFilas con sus índices que son outliers por columna:\")\n",
    "    for column, indices in outliers_por_columna.items():\n",
    "        print(f\"Columna '{column}': {list(zip(indices, exp.loc[indices][column]))}\")\n",
    "\n",
    "    # Lista para almacenar todos los índices de outliers\n",
    "    todos_indices_outliers = []\n",
    "\n",
    "    # Almacenar todos los índices de outliers en una lista\n",
    "    for indices in outliers_por_columna.values():\n",
    "        todos_indices_outliers.extend(indices)\n",
    "\n",
    "    # Convertir la lista de índices a un conjunto para eliminar duplicados\n",
    "    conjunto_indices_outliers = set(todos_indices_outliers)\n",
    "\n",
    "    # Lista ordenada de los índices únicos que no se repiten\n",
    "    indices_unicos_ordenados = sorted(list(conjunto_indices_outliers))\n",
    "\n",
    "    # Imprimir los índices únicos ordenados\n",
    "    print(\"Índices únicos de outliers ordenados:\")\n",
    "    print(len(indices_unicos_ordenados))\n",
    "    print(indices_unicos_ordenados)\n",
    "\n",
    "    # Construir un nuevo DataFrame que contenga las filas correspondientes a los índices únicos ordenados\n",
    "    dataframe_outliers = exp.loc[indices_unicos_ordenados]\n",
    "\n",
    "    # Imprimir el nuevo DataFrame de filas outliers únicas\n",
    "    print(\"Nuevo DataFrame de filas outliers únicas:\")\n",
    "    display(dataframe_outliers)\n",
    "    \n",
    "    # Agregar el DataFrame filtrado a la lista\n",
    "    exp_outliners.append(indices_unicos_ordenados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_sin_outliners = []\n",
    "\n",
    "# Iterar sobre los dataframes y aplicar el proceso a cada uno de ellos\n",
    "for exp, out in zip(experimentos, exp_outliners) :\n",
    "\n",
    "    exp_sin_outliner = exp.drop(out)\n",
    "\n",
    "    exp_sin_outliner = exp_sin_outliner.reset_index(drop=True)\n",
    "\n",
    "    exp_sin_outliners.append(exp_sin_outliner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMAX\n",
    "\n",
    "def normalizar_dataframe(dataframe):\n",
    "    # Selecciono las columnas a normalizar\n",
    "    columnas = ['Attention', 'Meditation', 'Delta', 'Theta', 'LowAlpha', 'HighAlpha', 'LowBeta', 'HighBeta', 'LowGamma', 'HighGamma']\n",
    "\n",
    "    # Normalizo las columnas seleccionadas\n",
    "    dataframe[columnas] = MinMaxScaler().fit_transform(dataframe[columnas])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Normalizar los 4 DataFrames diferentes de entrada y de salida\n",
    "experimento_1_minmax = normalizar_dataframe(exp_sin_outliners[0])\n",
    "experimento_2_minmax = normalizar_dataframe(exp_sin_outliners[1])\n",
    "experimento_3_minmax = normalizar_dataframe(exp_sin_outliners[2])\n",
    "experimento_4_minmax = normalizar_dataframe(exp_sin_outliners[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista que contendrá los DataFrames filtrados\n",
    "exp_filtrados = []\n",
    "\n",
    "# Iterar sobre los dataframes y aplicar la operación de filtrado a cada uno de ellos\n",
    "for exp in experimentos:\n",
    "    # Filtro el DataFrame para dejar fuera las columnas especificadas\n",
    "    exp_filtrado = exp.drop(['Timestamp', 'Key'], axis=1)\n",
    "    \n",
    "    # Agregar el DataFrame filtrado a la lista\n",
    "    exp_filtrados.append(exp_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@martacasdelg/c%C3%B3mo-identificar-y-tratar-outliers-con-python-bf7dd530fc3\n",
    "#https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "    \n",
    "# Filtro el DataFrame para dejar fuera las columnas especificadas\n",
    "experimento_4_filtrado = experimento_4_clean.drop(['Timestamp', 'Key'], axis=1)\n",
    "\n",
    "# Calcul0 el número de filas necesarias para mostrar dos boxplots por fila\n",
    "num_cols = len(experimento_4_filtrado.columns)\n",
    "num_rows = (num_cols) // 2\n",
    "\n",
    "# Definir el tamaño de la figura\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterar sobre cada columna del DataFrame filtrado\n",
    "for i, column in enumerate(experimento_4_filtrado.columns):\n",
    "    # Crear un subplot para cada columna\n",
    "    plt.subplot(num_rows, 2, i + 1)\n",
    "    \n",
    "    # Trazar un boxplot para la columna actual\n",
    "    sns.boxplot(data=experimento_4_filtrado, x=column)\n",
    "    \n",
    "    # Establecer el título del subplot como el nombre de la columna\n",
    "    plt.title(column)\n",
    "\n",
    "# Ajustar el diseño para evitar superposiciones\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimento_1_minmax.to_csv('../datos/experimento_1_nor.csv', index=False)\n",
    "experimento_2_minmax.to_csv('../datos/experimento_2_nor.csv', index=False)\n",
    "experimento_3_minmax.to_csv('../datos/experimento_3_nor.csv', index=False)\n",
    "experimento_4_minmax.to_csv('../datos/experimento_4_nor.csv', index=False)\n",
    "experimento_all_minmax = pd.concat([experimento_1_minmax, experimento_2_minmax, experimento_3_minmax, experimento_4_minmax])\n",
    "\n",
    "# Guardar el resultado en un archivo CSV\n",
    "experimento_all_minmax.to_csv('../datos/experimento_all_nor.csv', index=False) \n",
    "\n",
    "print()\n",
    "print(color.BOLD +  color.UNDERLINE + color.RED + \"Exporto todos los experimentos a un csv para tratarlo despues\" + color.END )\n",
    "print()\n",
    "print(color.BOLD +  color.UNDERLINE + color.RED + \"Exporto todos los experimentos a varios csv para tratarlos despues\" + color.END )\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49535abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimento_4_clean = pd.read_csv('../datos/experimento_4_clean.csv', sep=',')\n",
    "experimento_3_clean = pd.read_csv('../datos/experimento_3_clean.csv', sep=',')\n",
    "experimento_2_clean = pd.read_csv('../datos/experimento_2_clean.csv', sep=',')\n",
    "experimento_1_clean = pd.read_csv('../datos/experimento_1_clean.csv', sep=',')\n",
    "\n",
    "\n",
    "experimentos = [experimento_1_clean, experimento_2_clean, experimento_3_clean, experimento_4_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sacar los 4 por pantalla\n",
    "\n",
    "data = experimento_1_minmax\n",
    "# creating the dataset\n",
    "\n",
    "#y = dataframe['Key'].values\n",
    "#X = dataframe.drop(columns=['Key']).values\n",
    "\n",
    "labels1 = {'Left': \"04\", 'Right': \"02\" ,'Up': \"01\", 'Down': \"03\",'Nothing': \"00\"}\n",
    "\n",
    "#labels = set(data['Key'].values)\n",
    "values = data['Key'].values\n",
    "\n",
    "labels = data['Key']. replace(labels1)\n",
    "\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# Calcular la cantidad de cada elemento en la columna Key\n",
    "counts = data['Key'].value_counts()\n",
    "\n",
    "# Crear el barplot\n",
    "plt.bar(counts.index, counts.values, color ='maroon', width = 0.4)\n",
    "\n",
    "# Añadir etiquetas\n",
    "plt.xlabel('Elemento')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.title('Cantidad de elementos en la columna Key')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3584cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec96b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "# ==============================================================================\n",
    "modelo_1 = MLPClassifier(\n",
    "                hidden_layer_sizes=(5),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 1000,\n",
    "                random_state = SEED\n",
    "            )\n",
    "\n",
    "modelo_2 = MLPClassifier(\n",
    "                hidden_layer_sizes=(10),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 1000,\n",
    "                random_state = SEED\n",
    "            )\n",
    "\n",
    "modelo_3 = MLPClassifier(\n",
    "                hidden_layer_sizes=(20, 20),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 5000,\n",
    "                random_state = SEED\n",
    "            )\n",
    "\n",
    "modelo_4 = MLPClassifier(\n",
    "                hidden_layer_sizes=(50, 50, 50),\n",
    "                learning_rate_init=0.01,\n",
    "                solver = 'lbfgs',\n",
    "                max_iter = 5000,\n",
    "                random_state = SEED\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "y = experimento_1['Key'].values\n",
    "X = experimento_1.drop(columns=['Key']).values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelo_1.fit(X=X, y=y)\n",
    "\n",
    "modelo_2.fit(X=X, y=y)\n",
    "\n",
    "modelo_3.fit(X=X, y=y)\n",
    "\n",
    "modelo_4.fit(X=X, y=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ebd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''modelos_all= []\n",
    "\n",
    "for exp in experimentos_nor:\n",
    "    \n",
    "    modelos_all.append(evaluar_holdout(exp))\n",
    "\n",
    "datos_holdout = pd.DataFrame(modelos_all, columns=['Modelo1', 'Modelo2', 'Modelo3', 'Modelo4'])\n",
    "#creo los valores nuevo para los indices de las filas \n",
    "nuevos_indices = {0: 'Exp_1', 1: 'Exp_2', 2: 'Exp_3', 3: 'Exp_4'}\n",
    "# combio los valores anteriores en el dataframene con rename\n",
    "datos_holdout = datos_holdout.rename(index=nuevos_indices)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc82ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,8))\n",
    "axs = axs.flatten()\n",
    "num_points = 5  # o cualquier otro número más pequeño que 100\n",
    "grid_x1 = np.linspace(start=min(X[:, 0]), stop=max(X[:, 0]), num=num_points)\n",
    "grid_x2 = np.linspace(start=min(X[:, 1]), stop=max(X[:, 1]), num=num_points)\n",
    "grid_x3 = np.linspace(start=min(X[:, 2]), stop=max(X[:, 2]), num=num_points)\n",
    "grid_x4 = np.linspace(start=min(X[:, 3]), stop=max(X[:, 3]), num=num_points)\n",
    "grid_x5 = np.linspace(start=min(X[:, 4]), stop=max(X[:, 4]), num=num_points)\n",
    "grid_x6 = np.linspace(start=min(X[:, 5]), stop=max(X[:, 5]), num=num_points)\n",
    "grid_x7 = np.linspace(start=min(X[:, 6]), stop=max(X[:, 6]), num=num_points)\n",
    "grid_x8 = np.linspace(start=min(X[:, 7]), stop=max(X[:, 7]), num=num_points)\n",
    "grid_x9 = np.linspace(start=min(X[:, 8]), stop=max(X[:, 8]), num=num_points)\n",
    "grid_x10 = np.linspace(start=min(X[:, 9]), stop=max(X[:, 9]), num=num_points)\n",
    "\n",
    "# Usa los puntos reducidos para crear el grid\n",
    "xx, yy, zz, aa, bb, cc, dd, ee, ff, gg = np.meshgrid(grid_x1, grid_x2, grid_x3, grid_x4, grid_x5,\n",
    "                                                      grid_x6, grid_x7, grid_x8, grid_x9, grid_x10)\n",
    "X_grid = np.column_stack([xx.flatten(), yy.flatten(), zz.flatten(), aa.flatten(), bb.flatten(),\n",
    "                          cc.flatten(), dd.flatten(), ee.flatten(), ff.flatten(), gg.flatten()])\n",
    "\n",
    "\n",
    "for i, modelo in enumerate([modelo_1, modelo_2, modelo_3, modelo_4]):\n",
    "    \n",
    "    predicciones = modelo.predict(X_grid)\n",
    "    \n",
    "    for j in np.unique(predicciones):\n",
    "        axs[i].scatter(\n",
    "            x = X_grid[predicciones == j, 0],\n",
    "            y = X_grid[predicciones == j, 1], \n",
    "            c = plt.rcParams['axes.prop_cycle'].by_key()['color'][j],\n",
    "            #marker = 'o',\n",
    "            alpha = 0.3,\n",
    "            label= f\"Grupo {j}\"\n",
    "        )\n",
    "\n",
    "    for j in np.unique(y):\n",
    "        axs[i].scatter(\n",
    "            x = X[y == j, 0],\n",
    "            y = X[y == j, 1], \n",
    "            c = plt.rcParams['axes.prop_cycle'].by_key()['color'][j],\n",
    "            marker    = 'o',\n",
    "            edgecolor = 'black'\n",
    "        )\n",
    "        \n",
    "    axs[i].set_title(f\"Capas ocultas: {modelo.hidden_layer_sizes}\")\n",
    "    axs[i].axis('off')\n",
    "axs[0].legend();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd410d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clasificadores = [(MLPClassifier(max_iter=2000),\"MLP\"),\n",
    "                  (SVC(),\"SVC\")]\n",
    "\n",
    "clasificadores = [(modelo_1,\"Model1\"),\n",
    "                  (modelo_2,\"Model2\"),\n",
    "                  (modelo_3,\"Model3\"),\n",
    "                  (modelo_4,\"Model4\")]\n",
    "\n",
    "# Numero de folds\n",
    "n_folds = 2\n",
    "\n",
    "experimentos = [(\"Exp1\", experimento_1_nor), \n",
    "                (\"Exp2\", experimento_2_nor), \n",
    "                (\"Exp3\", experimento_3_nor), \n",
    "                (\"Exp4\", experimento_4_nor)]\n",
    "\n",
    "\n",
    "\n",
    "processed_datasets = []\n",
    "for name, dataset in experimentos:\n",
    "    y = dataset['Key'].values\n",
    "    X = dataset.drop(columns=['Timestamp','Key']).values\n",
    "    processed_datasets.append((name, X, y))\n",
    "\n",
    "#Defino los rangos de parámetros para SVM\n",
    "param_grid_svm = {\n",
    "    'C': np.linspace(0.01, 100000000, 10),  # Rango de valores para C\n",
    "    'gamma': np.linspace(0.00000001, 1000, 10)  # Rango de valores para gamma\n",
    "}\n",
    "\n",
    "# Defino los rangos de parámetros para MLP\n",
    "param_grid_mlp = {\n",
    "    'alpha': np.linspace(0.00001, 0.1, 10),  # Rango de valores para alpha\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,), (500,), (1000,)]  # Rango de valores para neuronas en la capa oculta\n",
    "}\n",
    "\n",
    "# Funcion para validacion cruzada\n",
    "def get_cv_results(clasificador, X, y, n_folds):\n",
    "    cv_scores = cross_val_score(clasificador, X, y, cv=n_folds)\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Funcion para validacion cruzada de Pipeline\n",
    "def get_pipeline_results(clasificador, X, y, n_folds):\n",
    "    #X = add_random_permutations(X)\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectFromModel(RandomForestClassifier(random_state=SEED), threshold=\"median\")),\n",
    "        ('classification', clasificador)\n",
    "    ])\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=n_folds)\n",
    "\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Genero las cuadriculas para a posteriori hallar los mejores parametros\n",
    "grid_search_mlp = GridSearchCV((MLPClassifier(max_iter=2000)), param_grid_mlp, cv=n_folds)\n",
    "grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=n_folds)\n",
    "\n",
    "clasificadores[0] = clasificadores[0] + (grid_search_mlp,)\n",
    "clasificadores[1] = clasificadores[1] + (grid_search_svm,)                              \n",
    "\n",
    "\n",
    "inicio_tiempo = time.time()\n",
    "\n",
    "# Realizo experimentos\n",
    "resultados_opt = []\n",
    "comparar = np.zeros(len(experimentos)*2)\n",
    "comparador = {}\n",
    "\n",
    "for clasificador, clasi_name, clasi_opt in clasificadores:\n",
    "    clasi_resultados = {}\n",
    "\n",
    "    for dataset_name, X, y in processed_datasets:\n",
    "        # Se dividen los datos. X_train_val sera el 90% de los datos y X_test el 10%\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=SEED, stratify=y, train_size=TRAIN)\n",
    "\n",
    "        # Se dividen los datos. X_train sera el 90% de los datos de X_train_val y X_val el restante 10%\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=SEED, stratify=y, train_size=TRAIN)\n",
    "        \n",
    "       \n",
    "        #Experimento 2\n",
    "        clasi_opt.fit(X_train, y_train)\n",
    "        print(clasi_name)\n",
    "        print(clasi_opt.best_params_) #imprimo los mejores parametros solo por curiosidad\n",
    "        optimized_scores = get_pipeline_results(clasi_opt.best_estimator_, X_val, y_val, n_folds)\n",
    "        \n",
    "        # Almaceno los resultados de los experimentos\n",
    "        #clasi_resultados[dataset_name + '_Sin_Opt'] = pipe_scores\n",
    "        clasi_resultados[dataset_name + '_Con_Opt'] = optimized_scores\n",
    "        \n",
    "    \n",
    "    # Almaceno los resultados\n",
    "    resultados_opt.append(clasi_resultados)\n",
    "\n",
    "fin_tiempo = time.time()\n",
    "\n",
    "tiempo_utilizado = fin_tiempo - inicio_tiempo\n",
    "\n",
    "# Convierto el tiempo a horas, minutos y segundos para saber cuanto tarda la ejecucion\n",
    "horas = int(tiempo_utilizado // 3600)\n",
    "minutos = int((tiempo_utilizado % 3600) // 60)\n",
    "segundos = int(tiempo_utilizado % 60)\n",
    "print(f\"Tiempo: {horas} : {minutos} : {segundos}\")\n",
    "\n",
    "# Creo el dataframe\n",
    "resultados_opt_df = pd.DataFrame(resultados_opt, index=['MPL', 'SVC'])\n",
    "                               \n",
    "                               \n",
    "#formateo las columnas e index\n",
    "resultados_opt_df.columns = pd.MultiIndex.from_tuples([\n",
    "                                        (\"Exp1\",\"Pipe_Opt\"),\n",
    "                                        (\"Exp2\",\"Pipe_Opt\"),\n",
    "                                        (\"Exp3\",\"Pipe_Opt\"),\n",
    "                                        (\"Exp4\",\"Pipe_Opt\")])\n",
    "\n",
    "\n",
    "display(resultados_opt_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_results(clasificador, X, y, n_folds):\n",
    "    cv_scores = accuracy_score(X, y)\n",
    "    report = classification_report(X, y)\n",
    "    matriz_confusion = confusion_matrix(X, y)\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "\n",
    "def get_cv_results(clasificador, X, y, n_folds):\n",
    "    cv_scores = cross_val_score(clasificador, X, y, cv=n_folds)\n",
    "    return cv_scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05e88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab096e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ef45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d7709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
